
3.30 : # Machine Translation # Kalchbrenner # CNN Encoder
============

1.'Neural Machine Translation in Linear Time', Kalchbrenner et al. 2017 (pid1)
	
	explanations TO BE ADDED

2.'Recurrent Continuous Translation Models', Kalchbrenner et al. 2013 (pid2)
	
	tried this one upon knowing that this paper suggests basic building block used at pid1. 
	Suggests CNN encoder which sums up the source sentence into a vector representation and sends it to the decoder(RCTM1)
	finished at April, 4th.


04.04 (Wed) : # Parsing Algorithm # Tomita's Parser
============
3.'An Efficient Augmented-Context-Free Parsing Algorithm', Masaru Tomita. 1987

	Start reading to understand parsing algorithm taught from the NLP class in Sogang University.
	
4. 'An Efficient Context-Free Parsing Algorithm', Jay Earley (1970)

	start reading on 4/10
